spring:
  application:
    name: datart-server

  main:
    banner-mode: off

  servlet:
    multipart:
      max-file-size: 1024MB
      max-request-size: 1024MB

  web:
    resources:
      static-locations: file:${datart.env.file-path}
      cache:
        cachecontrol:
          cache-public: true
          no-cache: true
          no-store: false

  messages:
    basename: i18n.datart_i18n

  jackson:
    date-format: yyyy-MM-dd HH:mm:ss
    time-zone: GMT+8
    serialization:
      FAIL_ON_EMPTY_BEANS: false
      FAIL_ON_UNWRAPPED_TYPE_IDENTIFIERS: false

  quartz:
    job-store-type: jdbc
    scheduler-name: DatartScheduleCluster
    properties:
      org:
        quartz:
          scheduler:
            instancName: DatartScheduleCluster
            instanceId: AUTO
          jobStore:
            class: org.quartz.impl.jdbcjobstore.JobStoreTX
            driverDelegateClass: org.quartz.impl.jdbcjobstore.StdJDBCDelegate
            isClustered: true
            clusterCheckinInterval: 5000
            useProperties: true
          threadPool:
            class: org.quartz.simpl.SimpleThreadPool
            threadCount: 10
            threadPriority: 5

mybatis:
  type-aliases-package: datart.core.entity
  mapper-locations: classpath*:mapper/**/*.xml
  configuration:
    use-generated-keys: true
    map-underscore-to-camel-case: true
    # log-impl: org.apache.ibatis.logging.stdout.StdOutImpl

logging:
  level:
    datart:
      core:
        mappers: info
      data.provider.jdbc: info
  config: config/logback.xml

shiro:
  web:
    enabled: false

datart:
  migration:
    enable: true

  server:
    path-prefix: /api/v1

  user:
    doris:
      default-compute-group: defaultComputeGroup

  ad-hoc:
    dir-name: "Ad-hoc Query SQLs" # adhoc 目录名称
    example-dir-name: "Example Query SQLs" # adhoc 示例目录名称

  folder:
    per-dir-name: "个人目录"
    example-dir-name: "示例"

  storyboard:
    per-dir-name: "个人目录"
    example-dir-name: "示例"

  task:
    queue_capacity: 100
    consumer:
      count: 4
    thread:
      pool:
        size: 20
        queue_capacity: 100
    execution:
      max_time: 10800000 # 3小时
    result:
      retention_days: 30
      retention_check_interval_min: 180 # 检查任务结果保留时间的间隔, 单位分钟

  spark:
    static-properties:
      - spark.app.name
      - spark.driver.cores
      - spark.driver.maxResultSize
      - spark.driver.memory
      - spark.driver.memoryOverhead
      - spark.driver.minMemoryOverhead
      - spark.driver.memoryOverheadFactor
      - spark.driver.resource
      - spark.driver.supervise
      - spark.driver.timeout
      - spark.driver.log.localDir
      - spark.driver.log.dfsDir
      - spark.driver.log.persistToDfs.enabled
      - spark.driver.log.layout
      - spark.driver.log.allowErasureCoding
      - spark.driver.log.redirectConsoleOutputs
      - spark.resources.discoveryPlugin
      - spark.executor.instances
      - spark.executor.memory
      - spark.executor.pyspark.memory
      - spark.executor.memoryOverhead
      - spark.executor.minMemoryOverhead
      - spark.executor.memoryOverheadFactor
      - spark.executor.resource
      - spark.executor.decommission.killInterval
      - spark.executor.decommission.forceKillTimeout
      - spark.executor.decommission.signal
      - spark.executor.maxNumFailures
      - spark.executor.failuresValidityInterval
      - spark.extraListeners
      - spark.local.dir
      - spark.logConf
      - spark.master
      - spark.submit.deployMode
      - spark.log.callerContext
      - spark.log.level
      - spark.decommission.enabled


server:
  port: 8080
  address: 0.0.0.0

  compression:
    enabled: true
    mime-types: application/javascript,application/json,application/xml,text/html,text/xml,text/plain,text/css,image/*

# Hadoop 配置
hadoop:
  username: hdfs
  task-result-dir: /datart/task_result/
  properties:
    fs.hdfs.impl: org.apache.hadoop.hdfs.DistributedFileSystem
    fs.defaultFS: hdfs://nameservice1
    ha.zookeeper.quorum: 10.68.36.115:2181,10.68.36.117:2181,10.68.37.160:2181
    dfs.client.failover.proxy.provider.nameservice1: org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
    dfs.ha.automatic-failover.enabled.nameservice1: true
    dfs.nameservices: nameservice1
    dfs.ha.namenodes.nameservice1: namenode112,namenode114
    dfs.namenode.rpc-address.nameservice1.namenode112: 10.68.36.115:8020
    dfs.namenode.rpc-address.nameservice1.namenode114: 10.68.36.117:8020
    dfs.namenode.servicerpc-address.nameservice1.namenode112: 10.68.36.115:8022
    dfs.namenode.servicerpc-address.nameservice1.namenode114: 10.68.36.117:8022
